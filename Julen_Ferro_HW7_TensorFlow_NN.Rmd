
library(keras)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)

rm(list=ls())

# Set working directory as needed

df <- read.csv("wifi_localization.csv")

# Seed the PRNG
set.seed(1122)
df <- df[sample(nrow(df)), ] # Shuffle, as all of the data in the .csv file
# is ordered by label!  

train_proportion <- 0.8
validation_proportion <- 0.2

index <- sample(1:nrow(df), nrow(df)*train_proportion)
leftovers  <- df[index, ]
df_test   <- df[-index, ]

dim(leftovers)

df_train <- leftovers

#index_2 <- sample(1:nrow(leftovers), validation_proportion*nrow(leftovers))

#df_validation <- leftovers[index_2, ]
#df_train <- leftovers[-index_2, ]


# --- Your code goes below ---

#DATA DESCRIPTION
#The CSV file contains 2001 rows, one of them is the header row
#Each row contains 8 columns (7 wifis and 1 room number)

# (a)




dim(df_train)
dim(df_test)
#dim(df_validation)

model <- rpart(room ~., data = df_train, method = 'class')
rpart.plot(model, extra = 101)

predictions <- predict(model, df_test, type = "class")

confu <- confusionMatrix(factor(predictions), reference = factor(df_test$room))

confu


# (b)
# Note that in (b) either use a new variable to store the model, or null out
# the variable that stored the model in (a) if you want to reuse that variable.
# The reason is that if you don't null it out, the model in (b) will have
# residual information left over from (a) and your results will not be quite
# accurate.



run <- function(neurons, df_train, df_test){
  NN_2 <- keras_model_sequential() %>%
    layer_dense(units = neurons, activation = 'relu' , input_shape = c(7)) %>%
    layer_dense(units = 5, activation = 'softmax' )
  
  # #We use 5 neurons in the final layer due to the fact that the 
  # #y_test.ohe column has 5 categorical values instead of 4, but 
  # one of them will be useless because it will have always null values
  
  # In order to be cleaner I have tried to eliminate the V1 Null column 
  # of y_test.ohe, but as it is a categorical column, the SELECT function 
  # does not work with that kind of data.
  
  #In the output layer, the neuron corresponding to V1 will always have 
  #null values
  
  NN_2 %>% 
    compile(loss = 'categorical_crossentropy',
            optimizer = "adam",
            metrics = c("accuracy"))
  
  X_train <- select(df_train, -room)
  y_train <- df_train$room
  
  X_test <- select(df_test, -room)
  y_test <- df_test$room
  
  y_train.ohe <- to_categorical(df_train$room)
  y_test.ohe <- to_categorical(df_test$room)
  
  
  NN_2 %>% fit(
    data.matrix(X_train), 
    y_train.ohe,
    epochs=100,
    batch_size=32,
    validation_split=0.20
  )
  
  NN_2 %>% evaluate(as.matrix(X_test), y_test.ohe)
  
  pred.prob <- predict(NN_2, as.matrix(X_test))
  pred.class <- apply(pred.prob, 1, function(x) which.max(x)-1) 
  
  confu_NN_2 <- confusionMatrix(as.factor(pred.class), as.factor(y_test))
  overall.accuracy <- confu_NN_2$overall['Accuracy']
  
  # print(overall.accuracy)
  # print(confu_NN_2)
  
  return(overall.accuracy)
  
}

#run(1, df_train, df_test)


# (c) 
accuracy <- 0.01 #Set a small value in order to let the program enter into the while loop
threshold <- 0.9 #Set the minimum accuracy in order to validate the NN structure, stop criterion
i <- 0
array <- c()

while(accuracy <= threshold){
   i <- i + 1
   accuracy <- run(i, df_train, df_test)
   array[i] <- accuracy
}

print(paste0("With ", i, " neurons, we reached the minimum accuracy of: ", threshold))

list <- 1:i

for(x in list){
  accu <- array[x]
  print(paste0("With ", x ," neurons the accuracy is: ", accu))
}



# (d)






